services:
  litellm-proxy:
    build:
      context: .
      dockerfile: Containerfile
    container_name: litellm-proxy
    networks:
      - agentllm
    ports:
      - "8890:8890"
    # Load environment variables from shared files
    env_file:
      - .env.shared          # Non-sensitive shared configuration
      - .env.secrets         # Sensitive credentials (git-ignored)
    volumes:
      # Mount local code for development (hot reload)
      - ./src/agentllm:/app/agentllm:rw,z
      # Persist database and workspace (local directory for easy access)
      - ./tmp/agent-data:/app/tmp:rw,z
      # Mount config from project root (for LiteLLM path resolution)
      - ./proxy_config.yaml:/app/proxy_config.yaml:ro,z
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8890/health/readiness"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    networks:
      - agentllm
    ports:
      - "3000:8080"
    # Load environment variables from shared files
    env_file:
      - .env.shared          # Non-sensitive shared configuration
      - .env.secrets         # Sensitive credentials (git-ignored)
    environment:
      # Override: Point to LiteLLM proxy (note: different variable name for OpenWebUI)
      # Compose uses OPENAI_API_BASE_URLS (plural) with the value from OPENAI_API_BASE_URL (singular)
      # Full container mode (both containerized): http://litellm-proxy:8890/v1
      OPENAI_API_BASE_URLS: "${OPENAI_API_BASE_URL:-http://litellm-proxy:8890/v1}"
      OPENAI_API_KEYS: "${LITELLM_MASTER_KEY:-sk-agno-test-key-12345}"
      ENABLE_SIGNUP: true

    # Note: For local proxy development mode, use nox -s dev_local_proxy instead
    # This runs the proxy on the host and only Open WebUI in a container
    volumes:
      - open-webui:/app/backend/data
    # depends_on:
    #   litellm-proxy:
    #     condition: service_healthy
    restart: unless-stopped

networks:
  agentllm:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: agentllm0
    enable_ipv6: false
    internal: false
    ipam:
      config:
        - subnet: 10.91.0.0/24

volumes:
  open-webui:
