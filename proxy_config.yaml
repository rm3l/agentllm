# LiteLLM Proxy Configuration for Agno Provider
# This configuration exposes Agno agents as OpenAI-compatible models

model_list:
  - model_name: agno/release-manager
    litellm_params:
      model: agno/release-manager
      custom_llm_provider: agno
    model_info:
      description: "Release management assistant for software releases, changelogs, and version planning"
      mode: chat

  - model_name: agno/rhai-roadmap-publisher
    litellm_params:
      model: agno/rhai-roadmap-publisher
      custom_llm_provider: agno
    model_info:
      description: "RHAI Roadmap Publisher agent for creating and publishing product roadmaps"
      mode: chat

  - model_name: agno/demo-agent
    litellm_params:
      model: agno/demo-agent
      custom_llm_provider: agno
    model_info:
      description: "Demo agent showcasing AgentLLM features with favorite color configuration and simple tools"
      mode: chat

  - model_name: agno/github-pr-prioritization
    litellm_params:
      model: agno/github-pr-prioritization
      custom_llm_provider: agno
    model_info:
      description: "GitHub PR Prioritization - Multi-factor scoring and intelligent review queue management"
      mode: chat

  - model_name: agno/sprint-reviewer
    litellm_params:
      model: agno/sprint-reviewer
      custom_llm_provider: agno
    model_info:
      description: "Sprint review assistant for creating team sprint reviews with metrics and issue details"
  - model_name: agno/rhdh-support
    litellm_params:
      model: agno/rhdh-support
      custom_llm_provider: agno
    model_info:
      description: "RHDH Support Focal - Monitoring support issues, customer cases, and team assignments"
      mode: chat

  # Google Gemini 2.5 models
  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      description: "Google Gemini 2.5 Pro - Most capable model"
      mode: chat

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: os.environ/GEMINI_API_KEY
    model_info:
      description: "Google Gemini 2.5 Flash - Fast and efficient"
      mode: chat

# General configuration
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY

  # Disable authentication for health checks (Kubernetes probes)
  disable_health_check_auth: true

  # Map OpenWebUI headers to LiteLLM user tracking
  # Enable ENABLE_FORWARD_USER_INFO_HEADERS=true in OpenWebUI
  user_header_name: X-OpenWebUI-User-Id
  user_header_mappings:
    - header_name: X-OpenWebUI-User-Id
      litellm_user_role: internal_user
    - header_name: X-OpenWebUI-User-Email
      litellm_user_role: customer

litellm_settings:
  drop_params: true # Drop unsupported params instead of erroring
  set_verbose: false # Disable verbose logging
  json_logs: true # Enable JSON logs for better log aggregation
  success_callback: []
  failure_callback: []
  # Register custom Agno provider handler
  custom_provider_map:
    - provider: "agno"
      custom_handler: custom_handler.agno_handler

# Server settings
server:
  host: 0.0.0.0
  port: 8890
