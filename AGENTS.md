# AGENTS.md

This file provides guidance to Claude Code when working with this repository.

## Overview

AgentLLM: LiteLLM custom provider exposing Agno agents via OpenAI-compatible API.

Architecture: `[Client] -> [LiteLLM Proxy :8890] -> [Agno Provider] -> [Agno Agent] -> [Gemini API]`

## Common Commands

```bash
# Testing
nox -s test                                    # Unit tests
nox -s integration                             # Integration tests (needs running proxy)
uv run pytest tests/test_custom_handler.py -v  # Specific test

# Development (most common)
nox -s proxy                                   # Terminal 1: local proxy with hot reload
nox -s dev_local_proxy                         # Terminal 2: OpenWebUI container

# Full container stack
nox -s dev                                     # Quick start (reuses images)
nox -s dev_build                               # Force rebuild
nox -s dev_logs                                # View logs

# Code quality
nox -s format                                  # Format
make lint                                      # Lint
```

## Critical Architecture Patterns

### Plugin System (NEW!)

AgentLLM now supports a **plugin-based architecture** for agents:

**AgentFactory Pattern:**
- Each agent implements an `AgentFactory` class
- Factories are registered via Python entry points in `pyproject.toml`
- Automatic discovery via `AgentRegistry.discover_agents()`

**Entry Point Registration:**
```toml
[project.entry-points."agentllm.agents"]
my-agent = "agentllm.agents.my_agent:MyAgentFactory"
```

**Benefits:**
- Agents as installable packages (separate repos possible)
- Auto-discovery at runtime (no manual imports needed)
- Metadata system for agent capabilities
- Clean separation of concerns

### Configurator Pattern (NEW!)

**AgentConfigurator** separates configuration management from agent execution:

**Responsibilities:**
- Configuration conversation (OAuth flows, token extraction)
- Toolkit management and collection
- Agent building with proper parameters
- Bound to user_id/session_id at construction

**BaseAgentWrapper** handles execution:
- Delegates to configurator for config management
- Provides run/arun interface
- Manages agent caching
- Handles streaming

**Key Classes:**
```python
from agentllm.agents.base import (
    AgentFactory,          # Factory for agent creation
    AgentRegistry,         # Plugin discovery
    AgentConfigurator,     # Configuration management
    BaseAgentWrapper,      # Execution interface
    BaseToolkitConfig,     # Toolkit configuration
)
```

### Custom Handler Path Resolution (GOTCHA!)

LiteLLM uses **file-based resolution**, not Python imports:

```
project_root/
â”œâ”€â”€ proxy_config.yaml          # LiteLLM loads from here
â”œâ”€â”€ custom_handler.py           # Stub that imports from src/
â””â”€â”€ src/agentllm/
    â””â”€â”€ custom_handler.py       # Actual implementation
```

**Why:** `custom_handler.agno_handler` in config â†’ LiteLLM looks for `./custom_handler.py` â†’ stub imports from `agentllm.custom_handler`

### Toolkit Configuration System

Base class: `BaseToolkitConfig` (`src/agentllm/agents/base/toolkit_config.py`)

Key methods:
- `is_configured(user_id)` - Check if toolkit is ready
- `extract_and_store_config(message, user_id)` - Parse and save credentials
- `get_config_prompt(user_id)` - Return prompt for missing config
- `get_toolkit(user_id)` - Return configured toolkit instance
- `is_required()` - Required toolkits prompt immediately, optional toolkits only when mentioned
- `check_authorization_request(message, user_id)` - Detect optional toolkit requests

### Streaming Format (CRITICAL!)

LiteLLM `CustomLLM` requires **GenericStreamingChunk**, NOT `ModelResponse`:

```python
{
    "text": "content",              # "text" not "content"!
    "finish_reason": "stop" or None,
    "is_finished": True or False,
    ...
}
```

### Knowledge Management System (RAG)

AgentLLM supports per-agent knowledge bases via **Retrieval-Augmented Generation (RAG)**. Each agent can have its own vector database and document collection for enhanced context.

**Quick Start**: Override `_get_knowledge_config()` in your agent configurator:

```python
def _get_knowledge_config(self) -> dict[str, Any] | None:
    return {
        "knowledge_path": "examples/my_knowledge",  # Path to MD/PDF/CSV files
        "table_name": "my_agent_knowledge"          # LanceDB table name
    }
```

That's it! Knowledge loading, indexing, and retrieval are handled automatically.

**Key Features**:
- Per-agent knowledge bases (no sharing between agent types)
- Lazy loading with persistence (fast startup, cached after first index)
- Hybrid search (vector + keyword)
- Optional (return `None` to disable)

ðŸ“– **See [docs/knowledge-management.md](../docs/knowledge-management.md) for complete documentation**

## Adding New Agents

### Modern Approach (Plugin System)

1. **Create AgentConfigurator** (`src/agentllm/agents/my_agent_configurator.py`):
   ```python
   from agentllm.agents.base import AgentConfigurator, BaseToolkitConfig

   class MyAgentConfigurator(AgentConfigurator):
       def _initialize_toolkit_configs(self) -> list[BaseToolkitConfig]:
           return []  # Add toolkit configs here

       def _build_agent_instructions(self) -> list[str]:
           return ["You are my agent.", "Your purpose is..."]

       def _get_agent_name(self) -> str:
           return "my-agent"

       def _get_agent_description(self) -> str:
           return "My agent description"

       def _get_knowledge_config(self) -> dict[str, Any] | None:
           """Override to enable RAG knowledge base (optional)."""
           return {
               "knowledge_path": "examples/my_agent_knowledge",
               "table_name": "my_agent_knowledge"
           }
           # Return None to disable knowledge for this agent
   ```

2. **Create BaseAgentWrapper** (`src/agentllm/agents/my_agent.py`):
   ```python
   from agentllm.agents.base import BaseAgentWrapper
   from .my_agent_configurator import MyAgentConfigurator

   class MyAgent(BaseAgentWrapper):
       def _create_configurator(self, user_id, session_id, shared_db, **kwargs):
           return MyAgentConfigurator(
               user_id=user_id,
               session_id=session_id,
               shared_db=shared_db,
               **kwargs
           )
   ```

3. **Create AgentFactory** (in same file):
   ```python
   from agentllm.agents.base import AgentFactory

   class MyAgentFactory(AgentFactory):
       @staticmethod
       def create_agent(shared_db, token_storage, user_id, session_id=None,
                       temperature=None, max_tokens=None, **kwargs):
           return MyAgent(
               shared_db=shared_db,
               user_id=user_id,
               session_id=session_id,
               temperature=temperature,
               max_tokens=max_tokens,
               **kwargs
           )

       @staticmethod
       def get_metadata():
           return {
               "name": "my-agent",
               "description": "My agent description",
               "mode": "chat",
               "requires_env": ["SOME_API_KEY"],
           }
   ```

4. **Register in `pyproject.toml`**:
   ```toml
   [project.entry-points."agentllm.agents"]
   my-agent = "agentllm.agents.my_agent:MyAgentFactory"
   ```

5. **Add to `proxy_config.yaml`**:
   ```yaml
   - model_name: agno/my-agent
     litellm_params:
       model: agno/my-agent
       custom_llm_provider: agno
   ```

6. **The agent will be auto-discovered** by `AgentRegistry` at runtime!

### Legacy Approach (Direct Import)

1. Create `src/agentllm/agents/my_agent.py`:
   - Follow wrapper pattern (see `demo_agent.py` for reference)
   - Use `shared_db` for session memory
   - Pass through `temperature` and `max_tokens` to model

2. Import in `src/agentllm/custom_handler.py`

3. Add to `proxy_config.yaml`:
   ```yaml
   - model_name: agno/my-agent
     litellm_params:
       model: agno/my-agent
       custom_llm_provider: agno
   ```

## Available Agents

### GitHub PR Prioritization (`agno/github-pr-prioritization`)

**Purpose**: Intelligent PR review queue management using multi-factor prioritization.

**Setup**:
1. Create GitHub personal access token:
   - Go to https://github.com/settings/tokens
   - Choose either:
     - **Fine-grained token** (recommended): Click "Generate new token (fine-grained)"
       - Select repository access and permissions
       - Token format: `github_pat_...`
     - **Classic token**: Click "Generate new token (classic)"
       - Select `repo` scope (full control of private repositories)
       - Token format: `ghp_...`
   - Copy the token
2. In chat: "My GitHub token is YOUR_TOKEN_HERE"
3. Agent validates and stores token securely

**Usage Examples**:
- "Show review queue for facebook/react"
- "What should I review next in owner/repo?"
- "Prioritize PRs in microsoft/vscode"
- "Analyze PR #12345 in owner/repo"
- "Team review velocity for the last 7 days"

**Prioritization Algorithm**:

Multi-factor scoring (0-80 scale) with weighted factors:
- **Age (25 pts)**: Older PRs get higher priority (capped at 7 days)
- **Size (20 pts)**: Smaller PRs score higher (penalized after 100 lines)
- **Activity (15 pts)**: Comments and review activity indicate importance
- **Labels (10 pts)**: urgent/hotfix/blocking/critical labels boost priority
- **Author (10 pts)**: Base score for all contributors

**Priority Tiers**:
- **CRITICAL (65-80)**: Hotfixes, urgent, blocking issues
- **HIGH (50-64)**: Aged PRs, active discussion
- **MEDIUM (35-49)**: Standard PRs ready for review
- **LOW (0-34)**: WIP, drafts

**Special Rules**:
- **Draft Exclusion**: Draft PRs are skipped unless requested
- **Label Boost**: urgent/hotfix/blocking/critical labels add 10 points
- **High Priority Labels**: high-priority/important labels add 7 points

**Output Format**:
Agent provides scored PR list with detailed breakdown, emoji indicators (ðŸ”´ Critical, ðŸŸ¡ Medium, ðŸŸ¢ Low), and clear recommendation for next review.

**Key Features**:
- Transparent score breakdowns showing exactly why each PR is prioritized
- Review queue filtering (exclude drafts, filter by state)
- Smart suggestions with reasoning and alternatives
- Repository velocity tracking (merged PRs, avg time to merge)
- Repository-scoped operations (can manage multiple repos)

**Implementation Details**:
- Toolkit: `GitHubToolkit` (`src/agentllm/tools/github_toolkit.py`)
- Configuration: `GitHubConfig` (`src/agentllm/agents/toolkit_configs/github_config.py`)
- Agent: `GitHubReviewAgent` (`src/agentllm/agents/github_pr_prioritization_agent.py`)
- Token storage: Database-backed via `TokenStorage.upsert_github_token()`
- Optional toolkit: Only prompts when GitHub/PRs mentioned

**Tools Available**:
- `list_prs(repo, state, limit)` - Simple markdown list of PRs with high-level info (no scoring)
- `prioritize_prs(repo, limit)` - Score and rank PRs with detailed breakdown
- `suggest_next_review(repo, reviewer)` - Smart recommendation with reasoning
- `get_repo_velocity(repo, days)` - Repository merge velocity metrics (all authors)

## Key Files

```
src/agentllm/
â”œâ”€â”€ custom_handler.py              # LiteLLM CustomLLM (caching, streaming)
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base/                      # NEW: Plugin system base classes
â”‚   â”‚   â”œâ”€â”€ factory.py             #   AgentFactory ABC
â”‚   â”‚   â”œâ”€â”€ registry.py            #   AgentRegistry (plugin discovery)
â”‚   â”‚   â”œâ”€â”€ configurator.py        #   AgentConfigurator (config management)
â”‚   â”‚   â”œâ”€â”€ wrapper.py             #   BaseAgentWrapper (execution interface)
â”‚   â”‚   â””â”€â”€ toolkit_config.py      #   BaseToolkitConfig (re-export)
â”‚   â”œâ”€â”€ release_manager.py         # Production agent wrapper
â”‚   â”œâ”€â”€ demo_agent.py              # Reference implementation
â”‚   â”œâ”€â”€ github_pr_prioritization_agent.py  # GitHub PR review agent
â”‚   â””â”€â”€ toolkit_configs/           # Toolkit config implementations
â”‚       â””â”€â”€ github_config.py       # GitHub token & toolkit config
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ github_toolkit.py          # GitHub PR review tools
â””â”€â”€ db/token_storage.py            # SQLite credential storage
```

## Environment

```bash
GEMINI_API_KEY=...                 # Required
LITELLM_MASTER_KEY=...             # Proxy auth (default: sk-agno-test-key-12345)
```

See `.env.secrets.template` for full config.

## TDD Workflow

1. Write failing test
2. Implement feature
3. `nox -s test`
4. Refactor

Always use `uv run` for Python commands.
